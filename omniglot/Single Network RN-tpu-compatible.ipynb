{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import task_generator_tpu as tgtpu\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import random\n",
    "# Hyper Parameters\n",
    "\n",
    "def get_cnn_output_dims(W, K, P, S):\n",
    "    return (int(W-K+2*P)/S)+1\n",
    "def cnn_final_output_dims(image_size):\n",
    "    dim1_calc = get_cnn_output_dims(image_size, 3, 0, 1)/2\n",
    "    dim2_calc = get_cnn_output_dims(dim1_calc, 3, 0, 1)/2\n",
    "    dim3_calc = get_cnn_output_dims(dim2_calc, 3, 1, 1)\n",
    "    final_output = get_cnn_output_dims(dim3_calc, 3, 1, 1)\n",
    "    return final_output\n",
    "def rn_dims_before_FCN(input_dims):\n",
    "    dim1_calc = get_cnn_output_dims(input_dims, 3, 1, 1)/2\n",
    "    final_output = get_cnn_output_dims(dim1_calc, 3, 1, 1)/2\n",
    "    return final_output\n",
    "\n",
    "CLASS_NUM = 5\n",
    "SAMPLE_NUM_PER_CLASS = 20\n",
    "QUERY_NUM_PER_CLASS = 15\n",
    "EPISODE = 1001\n",
    "TEST_EPISODE = 100\n",
    "LEARNING_RATE = 0.005\n",
    "NO_OF_TPU_CORES = 8\n",
    "GPU = 0\n",
    "HIDDEN_UNIT = 256\n",
    "IMAGE_SIZE = 28\n",
    "CHANNEL_DIM = 64\n",
    "DIMS = int(cnn_final_output_dims(IMAGE_SIZE))\n",
    "RN_DIMS = int(rn_dims_before_FCN(DIMS))\n",
    "FCN_SIZE = int(CHANNEL_DIM*(RN_DIMS**2))\n",
    "DATASET_FOLDER = '/home/caffe/achu/Data/china_drinks/image_data_train/cropped_images/'\n",
    "VALIDATION_SPLIT_PERCENTAGE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Plus_RNEncoder(nn.Module):\n",
    "    \"\"\"docstring for ClassName\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNN_Plus_RNEncoder, self).__init__()\n",
    "        self.cnn_layer1 = nn.Sequential(\n",
    "                        nn.Conv2d(1,64,kernel_size=3,padding=0),\n",
    "                        nn.BatchNorm2d(64, momentum=1, affine=True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2))\n",
    "        self.cnn_layer2 = nn.Sequential(\n",
    "                        nn.Conv2d(64,64,kernel_size=3,padding=0),\n",
    "                        nn.BatchNorm2d(64, momentum=1, affine=True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2))\n",
    "        self.cnn_layer3 = nn.Sequential(\n",
    "                        nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
    "                        nn.BatchNorm2d(64, momentum=1, affine=True),\n",
    "                        nn.ReLU())\n",
    "        self.cnn_layer4 = nn.Sequential(\n",
    "                        nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
    "                        nn.BatchNorm2d(64, momentum=1, affine=True),\n",
    "                        nn.ReLU())\n",
    "        \n",
    "        #RelationNetwork\n",
    "        self.rn_layer1 = nn.Sequential(\n",
    "                        nn.Conv2d(128,64,kernel_size=3,padding=1),\n",
    "                        nn.BatchNorm2d(64, momentum=1, affine=True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2))\n",
    "        self.rn_layer2 = nn.Sequential(\n",
    "                        nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
    "                        nn.BatchNorm2d(64, momentum=1, affine=True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2))\n",
    "        self.rn_fc1 = nn.Linear(FCN_SIZE,HIDDEN_UNIT)\n",
    "        self.rn_fc2 = nn.Linear(HIDDEN_UNIT,1)\n",
    "\n",
    "    def forward(self, samples, batches):\n",
    "        #print (samples.size(), batches.size())\n",
    "        sample_features = self.cnn_layer1(samples)\n",
    "        sample_features = self.cnn_layer2(sample_features)\n",
    "        sample_features = self.cnn_layer3(sample_features)\n",
    "        sample_features = self.cnn_layer4(sample_features)\n",
    "        cnn_features = sample_features\n",
    "        \n",
    "        sample_features = sample_features.view(CLASS_NUM,SAMPLE_NUM_PER_CLASS,CHANNEL_DIM, DIMS, DIMS)\n",
    "        sample_features = torch.sum(sample_features,1).squeeze(1)\n",
    "        \n",
    "        #batch features calculated from test for RN\n",
    "        batch_features = batches\n",
    "        batch_features = self.cnn_layer1(batch_features)\n",
    "        batch_features = self.cnn_layer2(batch_features)\n",
    "        batch_features = self.cnn_layer3(batch_features)\n",
    "        batch_features = self.cnn_layer4(batch_features)\n",
    "        cnn_batch_features = batch_features\n",
    "\n",
    "        # calculate relations\n",
    "        # each batch sample link to every samples to calculate relations\n",
    "        # to form a matrix for relation network\n",
    "        sample_features_ext = sample_features.unsqueeze(0).repeat((QUERY_NUM_PER_CLASS)*CLASS_NUM,1,1,1,1)\n",
    "        batch_features_ext = batch_features.unsqueeze(0).repeat(CLASS_NUM,1,1,1,1)\n",
    "        batch_features_ext = torch.transpose(batch_features_ext,0,1)\n",
    "        \n",
    "        #print ('batch_features_ext: ', batch_features_ext.size())\n",
    "        #print ('sample_features_ext: ', sample_features_ext.size())\n",
    "        \n",
    "        #[(no_of_query_images*(no_of_classes**2)), [CHANNEL_dim*2], dims, dims]\n",
    "        relation_pairs = torch.cat((sample_features_ext,batch_features_ext),2)\n",
    "        #print ('relation_pairs after concat: ', relation_pairs.size())\n",
    "        relation_pairs = relation_pairs.view(-1,CHANNEL_DIM*2,DIMS,DIMS)\n",
    "        #print ('relation_pairs before RN view: ', relation_pairs.size())\n",
    "        relation_pairs = self.rn_layer1(relation_pairs)\n",
    "        relation_pairs = self.rn_layer2(relation_pairs)\n",
    "        #print ('relation_pairs after rn layer 2: ', relation_pairs.size())\n",
    "        relation_pairs = relation_pairs.view(relation_pairs.size(0),-1)\n",
    "        #print ('relation_pairs before FCN: ', relation_pairs.size())\n",
    "        relation_pairs = F.relu(self.rn_fc1(relation_pairs))\n",
    "        relation_pairs = torch.sigmoid(self.rn_fc2(relation_pairs))\n",
    "        return relation_pairs\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()\n",
    "    elif classname.find('Linear') != -1:\n",
    "        n = m.weight.size(1)\n",
    "        m.weight.data.normal_(0, 0.01)\n",
    "        m.bias.data = torch.ones(m.bias.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_single_model():\n",
    "    # Step 1: init data folders\n",
    "    print(\"init data folders\", flush=True)\n",
    "    # init character folders for dataset construction\n",
    "    metatrain_character_folders,metatest_character_folders = tgtpu.china_drinks_sku_folders(DATASET_FOLDER, SAMPLE_NUM_PER_CLASS, QUERY_NUM_PER_CLASS, VALIDATION_SPLIT_PERCENTAGE)\n",
    "\n",
    "    # Step 2: init neural networks\n",
    "    print(\"init neural networks\")\n",
    "\n",
    "    relation_network = CNN_Plus_RNEncoder()\n",
    "    relation_network.apply(weights_init)\n",
    "    relation_network.cuda(GPU)\n",
    "    relation_network_optim = torch.optim.Adam(relation_network.parameters(),lr=LEARNING_RATE)\n",
    "    relation_network_scheduler = StepLR(relation_network_optim,step_size=100000,gamma=0.5)\n",
    "    \n",
    "    #if torch.cuda.device_count() > 1:\n",
    "    #    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    #    relation_network = nn.DataParallel(relation_network)\n",
    "    #else:\n",
    "    #    relation_network.cuda(GPU)\n",
    "\n",
    "\n",
    "    if os.path.exists(str(\"./models/omniglot_full_relation_network_\"+ str(CLASS_NUM) +\"way_\" + str(SAMPLE_NUM_PER_CLASS) +\"shot.pkl\")):\n",
    "        relation_network.load_state_dict(torch.load(str(\"./models/omniglot_full_relation_network_\"+ str(CLASS_NUM) +\"way_\" + str(SAMPLE_NUM_PER_CLASS) +\"shot.pkl\"), map_location='cuda:0'))\n",
    "        print(\"load full relation network success\")\n",
    "\n",
    "    # Step 3: build graph\n",
    "    print(\"Training...\")\n",
    "\n",
    "    last_accuracy = 0.0\n",
    "\n",
    "    for episode in range(EPISODE):\n",
    "\n",
    "        relation_network_scheduler.step(episode)\n",
    "\n",
    "        # init dataset\n",
    "        # sample_dataloader is to obtain previous samples for compare\n",
    "        # batch_dataloader is to batch samples for training\n",
    "        degrees = random.choice([0,90,180,270])\n",
    "        task = tgtpu.ChinaDrinksTask(metatrain_character_folders,CLASS_NUM,SAMPLE_NUM_PER_CLASS,QUERY_NUM_PER_CLASS)\n",
    "        sample_batch_dataloader = tgtpu.get_data_loader(task, image_size = IMAGE_SIZE, sample_num_per_class=SAMPLE_NUM_PER_CLASS, query_num_per_class = QUERY_NUM_PER_CLASS,train_shuffle=False, query_shuffle=True, rotation=degrees, num_workers = NO_OF_TPU_CORES)\n",
    "\n",
    "        # sample datas\n",
    "        samples,sample_labels,batches,batch_labels = sample_batch_dataloader.__iter__().next()        \n",
    "        relation_scores = relation_network(Variable(samples).cuda(GPU), Variable(batches).cuda(GPU))\n",
    "        relations = relation_scores.view(-1,CLASS_NUM)\n",
    "\n",
    "        mse = nn.MSELoss().cuda(GPU)\n",
    "        one_hot_labels = Variable(torch.zeros(QUERY_NUM_PER_CLASS*CLASS_NUM, CLASS_NUM).scatter_(1, batch_labels.view(-1,1), 1)).cuda(GPU)\n",
    "        loss = mse(relations,one_hot_labels)\n",
    "\n",
    "\n",
    "        # training\n",
    "\n",
    "        relation_network.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(relation_network.parameters(),0.5)\n",
    "        relation_network_optim.step()\n",
    "\n",
    "        if (episode+1)%100 == 0:\n",
    "                print(\"episode:\",episode+1,\"loss\",loss.data, flush = True)\n",
    "\n",
    "        if (episode+1)%5000 == 0:\n",
    "\n",
    "            # test\n",
    "            print(\"Testing...\")\n",
    "            total_rewards = 0\n",
    "\n",
    "            for i in range(TEST_EPISODE):\n",
    "                degrees = random.choice([0,90,180,270])\n",
    "                task = tgtpu.ChinaDrinksTask(metatest_character_folders,CLASS_NUM,SAMPLE_NUM_PER_CLASS,SAMPLE_NUM_PER_CLASS,)\n",
    "                sample_test_dataloader = tgtpu.get_data_loader(task,IMAGE_SIZE,sample_num_per_class=SAMPLE_NUM_PER_CLASS,query_num_per_class=QUERY_NUM_PER_CLASS,train_shuffle=False,test_shuffle= True,rotation=degrees, num_workers = NO_OF_TPU_CORES)        \n",
    "\n",
    "                sample_images, sample_labels, test_images, test_labels = sample_test_dataloader.__iter__().next()\n",
    "                \n",
    "                print (sample_images.size, test_images.size)\n",
    "\n",
    "                test_labels = test_labels.cuda()\n",
    "                \n",
    "                relations = relation_network(Variable(samples).cuda(GPU), Variable(batches).cuda(GPU)).view(-1,CLASS_NUM)\n",
    "\n",
    "                _,predict_labels = torch.max(relations.data,1)\n",
    "\n",
    "                rewards = [1 if predict_labels[j]==test_labels[j] else 0 for j in range(CLASS_NUM*SAMPLE_NUM_PER_CLASS)]\n",
    "\n",
    "                total_rewards += np.sum(rewards)\n",
    "\n",
    "            test_accuracy = total_rewards/1.0/CLASS_NUM/SAMPLE_NUM_PER_CLASS/TEST_EPISODE\n",
    "\n",
    "            print(\"validation accuracy:\",test_accuracy)\n",
    "\n",
    "            if test_accuracy > last_accuracy:\n",
    "\n",
    "                # save networks                \n",
    "                #torch.save(relation_network.state_dict(),str(\"./models/omniglot_full_relation_network_\"+ str(CLASS_NUM) +\"way_\" + str(SAMPLE_NUM_PER_CLASS) +\"shot.pkl\"))\n",
    "\n",
    "                print(\"save networks for episode:\",episode)\n",
    "\n",
    "                last_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init data folders\n",
      "Total SKUS in the dataset:  1255\n",
      "Total SKUS in the dataset with 35 samples:  906\n",
      "Train split SKUS:  816\n",
      "Val split SKUS:  90\n",
      "init neural networks\n",
      "Training...\n",
      "(100,) (75,)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/achu/LearningToCompare_FSL/omniglot/task_generator_tpu.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d1f781caaaf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_single_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-f677238e8e87>\u001b[0m in \u001b[0;36mmain_single_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# sample datas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batch_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mrelation_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelation_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelation_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCLASS_NUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/achu/anaconda2/envs/python3-pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/achu/anaconda2/envs/python3-pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;31m# prime the prefetch loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/achu/anaconda2/envs/python3-pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_put_indices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/achu/anaconda2/envs/python3-pytorch/lib/python3.7/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/achu/LearningToCompare_FSL/omniglot/task_generator_tpu.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;31m#train_batch, query_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_single_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
